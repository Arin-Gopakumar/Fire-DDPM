# cfgs/ddpm/ddpm_model.yaml

# Corrected Configuration for the DDPMLightning PyTorch Lightning Module
# This file defines the parameters for DDPMLightning's __init__ method.
# Removed the top-level 'model:' key to align with LightningCLI's expectation.

class_path: models.DDPMLightning.DDPMLightning  # Path to your DDPMLightning class
init_args:
  # --- Required arguments from DDPMLightning.__init__ ---
  n_channels: 115                     # Total input channels (e.g., 5 days * 23 channels/day)
  flatten_temporal_dimension: False   # DDPM does NOT flatten temporal dimension, it uses it as input
  pos_class_weight: 1.0               # Default positive class weight (adjust if needed)
  loss_function: "L2"                 # Dummy loss function name for BaseModel compatibility (actual loss is p_losses)
# --- DDPM specific parameters ---
  unet_params:
    image_size: 64
    in_target_channels: 1
    in_condition_channels: 115    # Total input channels (e.g., 5 days * 23 channels/day)
    model_channels: 64
    out_channels: 1
    num_res_blocks: 2
    channel_mult: [1, 2, 4, 8]
    time_emb_dim_mult: 4
    groups: 8
  
  diffusion_params:
    image_size: 64
    timesteps: 20
    beta_schedule_type: "linear"
    target_channels: 1

  optimizer_cfg:
    class_path: torch.optim.AdamW
    init_args:
      lr: 0.0001
      weight_decay: 0.01

  # Removed: loss_cfg as it's a dummy and CLI complains about its instantiation
  #loss_cfg:
    #class_path: torch.nn.Identity # Dummy loss, as p_losses handles actual loss
    #init_args: {} # No specific args for Identity

  metrics_cfg: {}
     # Placeholder, can be an empty dict {} if no specific configs needed here

  n_channels: 115

  pos_class_weight: 236